{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prisoner Escape Environment\n",
    "\n",
    "### Overview\n",
    "The **Prisoner Escape Environment** is a custom **grid-based** environment where a **prisoner** attempts to escape while avoiding a **guard**. The environment is implemented using **OpenAI Gym** and designed for **reinforcement learning (RL)** training.\n",
    "\n",
    "---\n",
    "\n",
    "### Grid Layout\n",
    "- The environment is a **7x7 grid**.\n",
    "- The **prisoner (`P`)** starts at a **fixed position** `(0,0)`.\n",
    "- The **escape point (`E`)** is placed **randomly** in a **specific area** `(2,2)` to `(5,5)`.\n",
    "- The **guard (`G`)** is placed **randomly** anywhere on the grid but **cannot** start at the same position as the prisoner or escape point.\n",
    "\n",
    "[['P' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' 'E' ' '] <br>\n",
    "[' ' ' ' ' ' 'G' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' ']]<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Actions\n",
    "The prisoner has **4 possible actions**:\n",
    "1. **Move Left** `(0)`: Decrease X position (if not at the left boundary).\n",
    "2. **Move Right** `(1)`: Increase X position (if not at the right boundary).\n",
    "3. **Move Up** `(2)`: Decrease Y position (if not at the top boundary).\n",
    "4. **Move Down** `(3)`: Increase Y position (if not at the bottom boundary).\n",
    "\n",
    "The **guard does not move**—it remains stationary.\n",
    "\n",
    "---\n",
    "\n",
    "### Observations\n",
    "The observation space consists of **three integer values**:\n",
    "1. **Prisoner’s Position**: `(x, y)` represented as a **single value** (`x + 7*y`).\n",
    "2. **Guard’s Position**: `(x, y)` represented similarly.\n",
    "3. **Escape Point’s Position**: `(x, y)` represented similarly.\n",
    "\n",
    "Since the grid is `7x7`, each position is encoded as a **single integer between 0 and 48**.\n",
    "\n",
    "- **Total observation space:** `shape = (3,)`\n",
    "\n",
    "---\n",
    "\n",
    "### Rewards\n",
    "- **+1** if the prisoner **reaches the escape point (`E`)**.\n",
    "- **-1** if the prisoner is **caught by the guard (`G`)**.\n",
    "- **0** if the prisoner is still trying to escape.\n",
    "- The episode **ends** when the prisoner **escapes or gets caught**.\n",
    "\n",
    "---\n",
    "\n",
    "### Episode Termination\n",
    "- The episode **terminates** if:\n",
    "  - The **prisoner reaches the escape point** (**success**).\n",
    "  - The **prisoner collides with the guard** (**failure**).\n",
    "  - The episode reaches **100 timesteps** (**truncation**).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of custom single environment in gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.utils import EzPickle, seeding\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvironment(gym.Env):\n",
    "    \"\"\"Custom Gym Environment for the prisoner escape task\"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"name\": \"custom_escape_environment_v0\"\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize environment parameters\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize coordinates and other environment variables\n",
    "        self.escape_y = None\n",
    "        self.escape_x = None\n",
    "        self.guard_y = None\n",
    "        self.guard_x = None\n",
    "        self.prisoner_y = None\n",
    "        self.prisoner_x = None\n",
    "        self.timestep = None\n",
    "        self.render_mode = None\n",
    "\n",
    "        # Define action space (4 possible actions)\n",
    "        self.action_space = spaces.Discrete(4)  # Prisoner has 4 actions\n",
    "\n",
    "        # Observation space - Prisoner's observation space\n",
    "        self.observation_space = spaces.Box(low=0, high=48, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to a starting state\"\"\"\n",
    "\n",
    "\n",
    "        self.prisoner_x = 0\n",
    "        self.prisoner_y = 0\n",
    "        self.escape_x = random.randint(2, 5)\n",
    "        self.escape_y = random.randint(2, 5)\n",
    "        \n",
    "        # Randomly place the guard\n",
    "        self.guard_x = random.randint(0, 6)\n",
    "        self.guard_y = random.randint(0, 6)\n",
    "        \n",
    "        # Ensure the guard does not spawn at the same location as the escape point or the prisoner\n",
    "        while (self.guard_x == self.prisoner_x and self.guard_y == self.prisoner_y) or \\\n",
    "              (self.guard_x == self.escape_x and self.guard_y == self.escape_y):\n",
    "            self.guard_x = random.randint(0, 6)\n",
    "            self.guard_y = random.randint(0, 6)\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Initialize observations for the prisoner\n",
    "        observations = np.array([\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y]\n",
    "            )\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment\"\"\"\n",
    "        # Execute prisoner action\n",
    "        if action == 0 and self.prisoner_x > 0:\n",
    "            self.prisoner_x -= 1\n",
    "        elif action == 1 and self.prisoner_x < 6:\n",
    "            self.prisoner_x += 1\n",
    "        elif action == 2 and self.prisoner_y > 0:\n",
    "            self.prisoner_y -= 1\n",
    "        elif action == 3 and self.prisoner_y < 6:\n",
    "            self.prisoner_y += 1\n",
    "\n",
    "        # Check if the prisoner has collided with the guard\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        if self.prisoner_x == self.guard_x and self.prisoner_y == self.guard_y:\n",
    "            # Prisoner caught by the guard\n",
    "            reward = -1\n",
    "            terminated = True\n",
    "        elif self.prisoner_x == self.escape_x and self.prisoner_y == self.escape_y:\n",
    "            # Prisoner escaped successfully\n",
    "            reward = 1\n",
    "            terminated = True\n",
    "\n",
    "        # Check truncation conditions (max steps)\n",
    "        truncation = False\n",
    "        if self.timestep > 100:\n",
    "            reward = 0\n",
    "            truncation = True\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        # Get the new observations\n",
    "        observations = np.array([\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y])\n",
    "\n",
    "        return observations, reward, terminated, truncation, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current state of the environment\"\"\"\n",
    "        grid = np.full((7, 7), \" \")\n",
    "        grid[self.prisoner_y, self.prisoner_x] = \"P\"\n",
    "        grid[self.guard_y, self.guard_x] = \"G\"\n",
    "        grid[self.escape_y, self.escape_x] = \"E\"\n",
    "        print(f\"{grid} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Ray-RLlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:15:14,805\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-02-06 13:15:15,446\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.tune.logger import UnifiedLogger\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "from ray.tune.logger import JsonLoggerCallback, CSVLoggerCallback, TBXLoggerCallback\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_agent_env = CustomEnvironment()\n",
    "\n",
    "\n",
    "# Register the single-agent environment\n",
    "register_env(\n",
    "    \"SingleAgentEnvironment\",\n",
    "    lambda _: single_agent_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .environment(\"SingleAgentEnvironment\")  # Use the registered environment name\n",
    "    .env_runners(\n",
    "        num_env_runners=1,  # Single environment\n",
    "        num_envs_per_env_runner=1,\n",
    "        batch_mode=\"complete_episodes\"\n",
    "    )\n",
    "    .framework(\"torch\")  # Use PyTorch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:15:31,316\tINFO worker.py:1816 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Define the stop criteria and logging settings\n",
    "stop_criteria = {\n",
    "    'training_iteration': 5  # Adjust as needed\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logging directory\n",
    "log_dir = f\"./tensorboard_logs_rlclass/single_example\"\n",
    "storage_path=\"file://\" + os.path.abspath(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-06 13:17:04</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:27.95        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.8/8.0 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_\n",
       "sample_reqs</th><th style=\"text-align: right;\">  num_remote_worker_re\n",
       "starts</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SingleAgentEnvironment_12378_00000</td><td>TERMINATED</td><td>127.0.0.1:16940</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         64.8528</td><td style=\"text-align: right;\">20263</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=16940)\u001b[0m Trainable.setup took 11.945 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m 2025-02-06 13:16:05,397\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000001)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000002)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000003)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:17:04,409\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example' in 0.0207s.\n",
      "2025-02-06 13:17:04,695\tINFO tune.py:1041 -- Total run time: 88.32 seconds (87.93 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Create a Tuner for training\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=ppo_config_single.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop=stop_criteria,\n",
    "        verbose=1,\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_frequency=1,  # Frequency of checkpoints (e.g., every .... iteration)\n",
    "            checkpoint_at_end=True  # Ensure final checkpoint at end\n",
    "        ),\n",
    "        storage_path=storage_path,  # Directory for TensorBoard logs\n",
    "        name=\"PPO_Training_Experiment_example\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default_policy': PPOTorchPolicy}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "checkpoint_dir = \"/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000004\" \n",
    "checkpoint_path = \"file://\" + os.path.abspath(checkpoint_dir)\n",
    "# Restore the model from the checkpoint\n",
    "policy = Policy.from_checkpoint(checkpoint_path)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Episode: 1\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'P' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'P' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'P' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' 'P' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'P']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'P' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' 'G' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1):  # You can change the number of episodes for testing\n",
    "    print(f\"Testing Episode: {episode + 1}\")\n",
    "    \n",
    "    obs,info = single_agent_env.reset()  # Reset the environment at the beginning of the episode\n",
    "    done = False\n",
    "    # episode_rewards = {\"Primary\": 0, \"Auxillary\": 0}\n",
    "\n",
    "    rewards = {} \n",
    "    container_states = {}\n",
    "    actions = {}\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        # Get actions from the trained model\n",
    "        s = 0\n",
    "        action = policy['default_policy'].compute_single_action(obs)[0]\n",
    "        \n",
    "        # Step the environment    \n",
    "        next_obs, reward, done, _ , infos = single_agent_env.step(action)\n",
    "        print('step',s )\n",
    "        single_agent_env.render()\n",
    "        print(reward)\n",
    "        s =+ 1\n",
    "        # Update observations\n",
    "        obs = next_obs\n",
    "# Shutdown Ray after testing is done\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Prison Escape Game\n",
    "\n",
    "## Objective\n",
    "The goal is to modify the original **Prison Escape Game** to a **multi-agent environment**. In this version, we have two agents:\n",
    "1. **Prisoner**: The agent trying to escape the prison.\n",
    "2. **Guard**: The agent trying to prevent the prisoner from escaping.\n",
    "\n",
    "## Environment Setup\n",
    "- The environment consists of a grid where the prisoner and the guard start at specific locations.\n",
    "- The **prisoner** tries to move towards a random escape point.\n",
    "- The **guard** tries to prevent the prisoner from reaching the escape point.\n",
    "\n",
    "### Agents\n",
    "- **Prisoner**: Can move in four directions (up, down, left, right).\n",
    "- **Guard**: Can also move in four directions (up, down, left, right).\n",
    "\n",
    "### State Space\n",
    "Each agent has its own state based on its position on the grid:\n",
    "- Prisoner’s state: `(prisoner_x, prisoner_y)`\n",
    "- Guard’s state: `(guard_x, guard_y)`\n",
    "- Escape point: `(escape_x, escape_y)`\n",
    "\n",
    "### Action Space\n",
    "Each agent has an action space represented as a discrete space with four possible actions:\n",
    "- **0**: Move left\n",
    "- **1**: Move right\n",
    "- **2**: Move up\n",
    "- **3**: Move down\n",
    "\n",
    "### Rewards\n",
    "- **Prisoner**: \n",
    "  - +1 if the prisoner reaches the escape point.\n",
    "  - -1 if caught by the guard.\n",
    "- **Guard**: \n",
    "  - +1 if the guard catches the prisoner.\n",
    "  - -1 if the prisoner escapes.\n",
    "\n",
    "### Termination Conditions\n",
    "- The game ends when:\n",
    "  - The prisoner reaches the escape point.\n",
    "  - The guard catches the prisoner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARL Environment in petting Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "\n",
    "from pettingzoo import ParallelEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRLEnvironment(ParallelEnv):\n",
    "    \"\"\"The metadata holds environment constants.\n",
    "\n",
    "    The \"name\" metadata allows the environment to be pretty printed.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\"\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The init method takes in environment arguments.\n",
    "\n",
    "        Should define the following attributes:\n",
    "        - escape x and y coordinates\n",
    "        - guard x and y coordinates\n",
    "        - prisoner x and y coordinates\n",
    "        - timestamp\n",
    "        - possible_agents\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.escape_y = None\n",
    "        self.escape_x = None\n",
    "        self.guard_y = None\n",
    "        self.guard_x = None\n",
    "        self.prisoner_y = None\n",
    "        self.prisoner_x = None\n",
    "        self.timestep = None\n",
    "        self.possible_agents = [\"guard\",\"prisoner\"]\n",
    "        self.render_mode = None\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset set the environment to a starting point.\n",
    "\n",
    "        It needs to initialize the following attributes:\n",
    "        - agents\n",
    "        - timestamp\n",
    "        - prisoner x and y coordinates\n",
    "        - guard x and y coordinates\n",
    "        - escape x and y coordinates\n",
    "        - observation\n",
    "        - infos\n",
    "\n",
    "        And must set up the environment so that render(), step(), and observe() can be called without issues.\n",
    "        \"\"\"\n",
    "        self.agents = copy(self.possible_agents)\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.prisoner_x = 0\n",
    "        self.prisoner_y = 0\n",
    "\n",
    "        self.guard_x = 6\n",
    "        self.guard_y = 6\n",
    "\n",
    "        self.escape_x = random.randint(2, 5)\n",
    "        self.escape_y = random.randint(2, 5)\n",
    "\n",
    "        observations = {\n",
    "            a: (\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y,\n",
    "            )\n",
    "            for a in self.agents\n",
    "        }\n",
    "\n",
    "        # Get dummy infos. Necessary for proper parallel_to_aec conversion\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Takes in an action for the current agent (specified by agent_selection).\n",
    "\n",
    "        Needs to update:\n",
    "        - prisoner x and y coordinates\n",
    "        - guard x and y coordinates\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - rewards\n",
    "        - timestamp\n",
    "        - infos\n",
    "\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        # Execute actions\n",
    "        prisoner_action = actions[\"prisoner\"]\n",
    "        guard_action = actions[\"guard\"]\n",
    "\n",
    "        if prisoner_action == 0 and self.prisoner_x > 0:\n",
    "            self.prisoner_x -= 1\n",
    "        elif prisoner_action == 1 and self.prisoner_x < 6:\n",
    "            self.prisoner_x += 1\n",
    "        elif prisoner_action == 2 and self.prisoner_y > 0:\n",
    "            self.prisoner_y -= 1\n",
    "        elif prisoner_action == 3 and self.prisoner_y < 6:\n",
    "            self.prisoner_y += 1\n",
    "\n",
    "        if guard_action == 0 and self.guard_x > 0:\n",
    "            self.guard_x -= 1\n",
    "        elif guard_action == 1 and self.guard_x < 6:\n",
    "            self.guard_x += 1\n",
    "        elif guard_action == 2 and self.guard_y > 0:\n",
    "            self.guard_y -= 1\n",
    "        elif guard_action == 3 and self.guard_y < 6:\n",
    "            self.guard_y += 1\n",
    "\n",
    "        # Check termination conditions\n",
    "        terminations = {a: False for a in self.agents}\n",
    "        rewards = {a: 0 for a in self.agents}\n",
    "        if self.prisoner_x == self.guard_x and self.prisoner_y == self.guard_y:\n",
    "            rewards = {\"prisoner\": -1, \"guard\": 1}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "\n",
    "        elif self.prisoner_x == self.escape_x and self.prisoner_y == self.escape_y:\n",
    "            rewards = {\"prisoner\": 1, \"guard\": -1}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "\n",
    "        # Check truncation conditions (overwrites termination conditions)\n",
    "        truncations = {a: False for a in self.agents}\n",
    "        if self.timestep > 100:\n",
    "            rewards = {\"prisoner\": 0, \"guard\": 0}\n",
    "            truncations = {\"prisoner\": True, \"guard\": True}\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        # Get observations\n",
    "        observations = {\n",
    "            a: (\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y,\n",
    "            )\n",
    "            for a in self.agents\n",
    "        }\n",
    "\n",
    "        # Get dummy infos (not used in this example)\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        # if any(terminations.values()) or any(truncations.values()):\n",
    "            # self.agents = []\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the environment.\"\"\"\n",
    "        grid = np.full((7, 7), \" \")\n",
    "        grid[self.prisoner_y, self.prisoner_x] = \"P\"\n",
    "        grid[self.guard_y, self.guard_x] = \"G\"\n",
    "        grid[self.escape_y, self.escape_x] = \"E\"\n",
    "        print(f\"{grid} \\n\")\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return MultiDiscrete([7 * 7] * 3)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_marl = MRLEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(\n",
    "    \"MRLEnvironment\",\n",
    "    lambda _: ParallelPettingZooEnv(env_marl),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_ = {name: (None, env_marl.observation_space(name), env_marl.action_space(name), {}) for name in env_marl.possible_agents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"MRLEnvironment\")  # Use the registered environment name\n",
    "    .env_runners(\n",
    "        num_env_runners=1,  # Replaces num_rollout_workers\n",
    "        num_envs_per_env_runner=1,  # Replaces num_envs_per_worker\n",
    "        batch_mode=\"complete_episodes\",  # Ensure full episodes are processed\n",
    "        rollout_fragment_length='auto',  # Length of an episode\n",
    "    )\n",
    "    .framework(\"torch\")  # Use PyTorch\n",
    "    .multi_agent(\n",
    "        policies=policies_,\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: agent_id)  # Maps agent IDs directly to policies\n",
    "    \n",
    "    .training(\n",
    "        model = {\n",
    "        'vf_share_layers':True,},\n",
    "        train_batch_size=40,  \n",
    "        minibatch_size=4,\n",
    "        gamma=0.2,\n",
    "        use_critic=True,  # Use a value function for advantage estimation\n",
    "        use_gae=True,  # Generalized Advantage Estimation\n",
    "        lambda_=0.5,  # GAE parameter for bias-variance trade-off\n",
    "        entropy_coeff_schedule=[\n",
    "            (0, 0.1),       # Start with high exploration\n",
    "            (50000, 0.1),   # Keep high entropy until iteration 50\n",
    "            (75000, 0.075),  # Start decreasing entropy around iteration 75\n",
    "            (100000, 0.05), # Reach low entropy by iteration 100\n",
    "        ],  # Encourage exploration with lower entropy cost\n",
    "        vf_loss_coeff=5.0,  # Coefficient for value function los\n",
    "        grad_clip=0.5,  # Clip gradients to stabilize training\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Define the stop criteria and logging settings\n",
    "stop_criteria = {\n",
    "    'training_iteration':5 # Specify based on your needs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logging directory\n",
    "log_dir = f\"./tensorboard_logs/multi\"\n",
    "storage_path=\"file://\" + os.path.abspath(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tuner for training with TensorBoard logging and checkpointing\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",  # Specify the RLlib algorithm\n",
    "    param_space=ppo_config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop=stop_criteria,\n",
    "        verbose=1,\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_frequency=5,  # Frequency of checkpoints (e.g., every iteration)\n",
    "            checkpoint_at_end=True  # Ensure final checkpoint at end\n",
    "        ),\n",
    "        storage_path=storage_path,  # Directory for TensorBoard logs\n",
    "        name=\"PPO_Training_Experiment_multi\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "results = tuner.fit()\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
