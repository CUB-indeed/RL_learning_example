{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoritical Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prisoner Escape Environment\n",
    "\n",
    "### Overview\n",
    "The **Prisoner Escape Environment** is a custom **grid-based** environment where a **prisoner** attempts to escape while avoiding a **guard**. The environment is implemented using **OpenAI Gym** and designed for **reinforcement learning (RL)** training.\n",
    "\n",
    "---\n",
    "\n",
    "### Grid Layout\n",
    "- The environment is a **7x7 grid**.\n",
    "- The **prisoner (`P`)** starts at a **fixed position** `(0,0)`.\n",
    "- The **escape point (`E`)** is placed **randomly** in a **specific area** `(2,2)` to `(5,5)`.\n",
    "- The **guard (`G`)** is placed **randomly** anywhere on the grid but **cannot** start at the same position as the prisoner or escape point.\n",
    "\n",
    "[['P' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' 'E' ' '] <br>\n",
    "[' ' ' ' ' ' 'G' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' '] <br>\n",
    "[' ' ' ' ' ' ' ' ' ' ' ']]<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Actions\n",
    "The prisoner has **4 possible actions**:\n",
    "1. **Move Left** `(0)`: Decrease X position (if not at the left boundary).\n",
    "2. **Move Right** `(1)`: Increase X position (if not at the right boundary).\n",
    "3. **Move Up** `(2)`: Decrease Y position (if not at the top boundary).\n",
    "4. **Move Down** `(3)`: Increase Y position (if not at the bottom boundary).\n",
    "\n",
    "The **guard does not move**—it remains stationary.\n",
    "\n",
    "---\n",
    "\n",
    "### Observations\n",
    "The observation space consists of **three integer values**:\n",
    "1. **Prisoner’s Position**: `(x, y)` represented as a **single value** (`x + 7*y`).\n",
    "2. **Guard’s Position**: `(x, y)` represented similarly.\n",
    "3. **Escape Point’s Position**: `(x, y)` represented similarly.\n",
    "\n",
    "Since the grid is `7x7`, each position is encoded as a **single integer between 0 and 48**.\n",
    "\n",
    "- **Total observation space:** `shape = (3,)`\n",
    "\n",
    "---\n",
    "\n",
    "### Rewards\n",
    "- **+1** if the prisoner **reaches the escape point (`E`)**.\n",
    "- **-1** if the prisoner is **caught by the guard (`G`)**.\n",
    "- **0** if the prisoner is still trying to escape.\n",
    "- The episode **ends** when the prisoner **escapes or gets caught**.\n",
    "\n",
    "---\n",
    "\n",
    "### Episode Termination\n",
    "- The episode **terminates** if:\n",
    "  - The **prisoner reaches the escape point** (**success**).\n",
    "  - The **prisoner collides with the guard** (**failure**).\n",
    "  - The episode reaches **100 timesteps** (**truncation**).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of custom single environment in gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.utils import EzPickle, seeding\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvironment(gym.Env):\n",
    "    \"\"\"Custom Gym Environment for the prisoner escape task\"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"name\": \"custom_escape_environment_v0\"\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize environment parameters\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize coordinates and other environment variables\n",
    "        self.escape_y = None\n",
    "        self.escape_x = None\n",
    "        self.guard_y = None\n",
    "        self.guard_x = None\n",
    "        self.prisoner_y = None\n",
    "        self.prisoner_x = None\n",
    "        self.timestep = None\n",
    "        self.render_mode = None\n",
    "\n",
    "        # Define action space (4 possible actions)\n",
    "        self.action_space = spaces.Discrete(4)  # Prisoner has 4 actions\n",
    "\n",
    "        # Observation space - Prisoner's observation space\n",
    "        self.observation_space = spaces.Box(low=0, high=48, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to a starting state\"\"\"\n",
    "\n",
    "\n",
    "        self.prisoner_x = 0\n",
    "        self.prisoner_y = 0\n",
    "        self.escape_x = random.randint(2, 5)\n",
    "        self.escape_y = random.randint(2, 5)\n",
    "        \n",
    "        # Randomly place the guard\n",
    "        self.guard_x = random.randint(0, 6)\n",
    "        self.guard_y = random.randint(0, 6)\n",
    "        \n",
    "        # Ensure the guard does not spawn at the same location as the escape point or the prisoner\n",
    "        while (self.guard_x == self.prisoner_x and self.guard_y == self.prisoner_y) or \\\n",
    "              (self.guard_x == self.escape_x and self.guard_y == self.escape_y):\n",
    "            self.guard_x = random.randint(0, 6)\n",
    "            self.guard_y = random.randint(0, 6)\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Initialize observations for the prisoner\n",
    "        observations = np.array([\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y]\n",
    "            )\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment\"\"\"\n",
    "        # Execute prisoner action\n",
    "        if action == 0 and self.prisoner_x > 0:\n",
    "            self.prisoner_x -= 1\n",
    "        elif action == 1 and self.prisoner_x < 6:\n",
    "            self.prisoner_x += 1\n",
    "        elif action == 2 and self.prisoner_y > 0:\n",
    "            self.prisoner_y -= 1\n",
    "        elif action == 3 and self.prisoner_y < 6:\n",
    "            self.prisoner_y += 1\n",
    "\n",
    "        # Check if the prisoner has collided with the guard\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        if self.prisoner_x == self.guard_x and self.prisoner_y == self.guard_y:\n",
    "            # Prisoner caught by the guard\n",
    "            reward = -1\n",
    "            terminated = True\n",
    "        elif self.prisoner_x == self.escape_x and self.prisoner_y == self.escape_y:\n",
    "            # Prisoner escaped successfully\n",
    "            reward = 1\n",
    "            terminated = True\n",
    "\n",
    "        # Check truncation conditions (max steps)\n",
    "        truncation = False\n",
    "        if self.timestep > 100:\n",
    "            reward = 0\n",
    "            truncation = True\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        # Get the new observations\n",
    "        observations = np.array([\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y])\n",
    "\n",
    "        return observations, reward, terminated, truncation, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current state of the environment\"\"\"\n",
    "        grid = np.full((7, 7), \" \")\n",
    "        grid[self.prisoner_y, self.prisoner_x] = \"P\"\n",
    "        grid[self.guard_y, self.guard_x] = \"G\"\n",
    "        grid[self.escape_y, self.escape_x] = \"E\"\n",
    "        print(f\"{grid} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Ray-RLlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:15:14,805\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-02-06 13:15:15,446\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.tune.logger import UnifiedLogger\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "from ray.tune.logger import JsonLoggerCallback, CSVLoggerCallback, TBXLoggerCallback\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_agent_env = CustomEnvironment()\n",
    "\n",
    "\n",
    "# Register the single-agent environment\n",
    "register_env(\n",
    "    \"SingleAgentEnvironment\",\n",
    "    lambda _: single_agent_env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .environment(\"SingleAgentEnvironment\")  # Use the registered environment name\n",
    "    .env_runners(\n",
    "        num_env_runners=1,  # Single environment\n",
    "        num_envs_per_env_runner=1,\n",
    "        batch_mode=\"complete_episodes\"\n",
    "    )\n",
    "    .framework(\"torch\")  # Use PyTorch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:15:31,316\tINFO worker.py:1816 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Define the stop criteria and logging settings\n",
    "stop_criteria = {\n",
    "    'training_iteration': 5  # Adjust as needed\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logging directory\n",
    "log_dir = f\"./tensorboard_logs_rlclass/single_example\"\n",
    "storage_path=\"file://\" + os.path.abspath(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-06 13:17:04</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:27.95        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.8/8.0 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_\n",
       "sample_reqs</th><th style=\"text-align: right;\">  num_remote_worker_re\n",
       "starts</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SingleAgentEnvironment_12378_00000</td><td>TERMINATED</td><td>127.0.0.1:16940</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         64.8528</td><td style=\"text-align: right;\">20263</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=16940)\u001b[0m Trainable.setup took 11.945 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m 2025-02-06 13:16:05,397\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000001)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000002)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000003)\n",
      "\u001b[36m(PPO pid=16940)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:17:04,409\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example' in 0.0207s.\n",
      "2025-02-06 13:17:04,695\tINFO tune.py:1041 -- Total run time: 88.32 seconds (87.93 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Create a Tuner for training\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=ppo_config_single.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop=stop_criteria,\n",
    "        verbose=1,\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_frequency=1,  # Frequency of checkpoints (e.g., every .... iteration)\n",
    "            checkpoint_at_end=True  # Ensure final checkpoint at end\n",
    "        ),\n",
    "        storage_path=storage_path,  # Directory for TensorBoard logs\n",
    "        name=\"PPO_Training_Experiment_example\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "results = tuner.fit()\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default_policy': PPOTorchPolicy}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "checkpoint_dir = \"/Users/sakshisharma/Desktop/rl_scheduling/tensorboard_logs_rlclass/single_example/PPO_Training_Experiment_example/PPO_SingleAgentEnvironment_12378_00000_0_2025-02-06_13-15-36/checkpoint_000004\" \n",
    "checkpoint_path = \"file://\" + os.path.abspath(checkpoint_dir)\n",
    "# Restore the model from the checkpoint\n",
    "policy = Policy.from_checkpoint(checkpoint_path)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Episode: 1\n",
      "step 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' ' ' ' ' 'P' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "0\n",
      "step 0\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'G' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' 'E' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1):  # You can change the number of episodes for testing\n",
    "    print(f\"Testing Episode: {episode + 1}\")\n",
    "    \n",
    "    obs,info = single_agent_env.reset()  # Reset the environment at the beginning of the episode\n",
    "    done = False\n",
    "    # episode_rewards = {\"Primary\": 0, \"Auxillary\": 0}\n",
    "\n",
    "    rewards = {} \n",
    "    container_states = {}\n",
    "    actions = {}\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        # Get actions from the trained model\n",
    "        s = 0\n",
    "        action = policy['default_policy'].compute_single_action(obs)[0]\n",
    "        \n",
    "        # Step the environment    \n",
    "        next_obs, reward, done, _ , infos = single_agent_env.step(action)\n",
    "        print('step',s )\n",
    "        single_agent_env.render()\n",
    "        print(reward)\n",
    "        s =+ 1\n",
    "        # Update observations\n",
    "        obs = next_obs\n",
    "# Shutdown Ray after testing is done\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially Observable Markov Games (POMG) and Multi-Agent RL\n",
    "\n",
    "## Partially Observable Markov Games (POMG)\n",
    "\n",
    "A **Partially Observable Markov Game (POMG)** extends the **Markov Decision Process (MDP)** to multi-agent settings where agents have limited information about the environment.\n",
    "\n",
    "A POMG is defined by:\n",
    "- **N agents** interacting in an environment.\n",
    "- Each agent **i** has a private observation \\( o_i \\) derived from the state \\( s \\).\n",
    "- Each agent selects an action \\( a_i \\) based on \\( o_i \\).\n",
    "- The environment transitions to a new state \\( s' \\) based on all agents' actions.\n",
    "- Agents receive individual rewards \\( R_i \\), which may be cooperative or competitive.\n",
    "\n",
    "## Multi-Agent Reinforcement Learning (MARL)\n",
    "\n",
    "In MARL, multiple agents learn simultaneously, affecting each other’s learning process. The two primary settings are:\n",
    "1. **Cooperative**: Agents share rewards and work towards a common goal.\n",
    "2. **Competitive**: Agents have conflicting objectives (e.g., adversarial games).\n",
    "\n",
    "Common MARL algorithms:\n",
    "- **Independent Q-Learning**: Each agent learns its own Q-function, treating others as part of the environment.\n",
    "- **Centralized Training, Decentralized Execution (CTDE)**: Agents train with shared knowledge but act independently.\n",
    "- **Multi-Agent Deep Deterministic Policy Gradient (MADDPG)**: An extension of DDPG for multi-agent settings.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Prisoner Escape Environment  \n",
    "\n",
    "###  Overview  \n",
    "The **Multi-Agent Prisoner Escape Environment** is extention of previous environment where two agents—the **Prisoner** and the **Guard**—interact with the goal of either escaping or preventing the escape. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Environment Details  \n",
    "\n",
    "- **Initial Positions:**  \n",
    "  - **Prisoner:** Always starts at **(0,0)**.  \n",
    "  - **Escape Point:** Randomly placed between **(2,2) to (5,5)**.  \n",
    "  - **Guard:** Randomly placed **anywhere** except the escape point or prisoner’s start location.  \n",
    "\n",
    "- **Observations (for each agent):**  \n",
    "  - **Prisoner:**  \n",
    "    - Own position `(x, y)`  \n",
    "    - Guard’s position `(x, y)`  \n",
    "    - Escape point position `(x, y)`  \n",
    "  - **Guard:**  \n",
    "    - Own position `(x, y)`  \n",
    "    - Prisoner’s position `(x, y)`  \n",
    "\n",
    "\n",
    "\n",
    "##  Game Rules & Termination  \n",
    "The game ends when **either**:  \n",
    "1. **The prisoner reaches the escape point** → **Prisoner wins**   \n",
    "2. **The guard catches the prisoner** → **Guard wins** \n",
    "3. **Maximum steps (100) reached** → **Game ends in a draw**   \n",
    "\n",
    "### **Rewards:**  \n",
    "- **Prisoner:**  \n",
    "  - `+1` for escaping  \n",
    "  - `-1` if caught  \n",
    "- **Guard:**  \n",
    "  - `+1` for catching the prisoner   \n",
    "  - `-1` if the prisoner escapes   \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARL Environment in petting Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "import supersuit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRLEnvironment(ParallelEnv):\n",
    "    \"\"\"The metadata holds environment constants.\n",
    "\n",
    "    The \"name\" metadata allows the environment to be pretty printed.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\"\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The init method takes in environment arguments.\n",
    "\n",
    "        Should define the following attributes:\n",
    "        - escape x and y coordinates\n",
    "        - guard x and y coordinates\n",
    "        - prisoner x and y coordinates\n",
    "        - timestamp\n",
    "        - possible_agents\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.escape_y = None\n",
    "        self.escape_x = None\n",
    "        self.guard_y = None\n",
    "        self.guard_x = None\n",
    "        self.prisoner_y = None\n",
    "        self.prisoner_x = None\n",
    "        self.timestep = None\n",
    "        self.possible_agents = [\"guard\",\"prisoner\"]\n",
    "        self.render_mode = None\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset set the environment to a starting point.\n",
    "\n",
    "        It needs to initialize the following attributes:\n",
    "        - agents\n",
    "        - timestamp\n",
    "        - prisoner x and y coordinates\n",
    "        - guard x and y coordinates\n",
    "        - escape x and y coordinates\n",
    "        - observation\n",
    "        - infos\n",
    "\n",
    "        And must set up the environment so that render(), step(), and observe() can be called without issues.\n",
    "        \"\"\"\n",
    "        self.agents = copy(self.possible_agents)\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.prisoner_x = 0\n",
    "        self.prisoner_y = 0\n",
    "\n",
    "        self.guard_x = 6\n",
    "        self.guard_y = 6\n",
    "\n",
    "        self.escape_x = random.randint(2, 5)\n",
    "        self.escape_y = random.randint(2, 5)\n",
    "\n",
    "        observations = {\n",
    "            a: (\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y,\n",
    "            )\n",
    "            for a in self.agents\n",
    "        }\n",
    "\n",
    "        # Get dummy infos. Necessary for proper parallel_to_aec conversion\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Takes in an action for the current agent (specified by agent_selection).\n",
    "\n",
    "        Needs to update:\n",
    "        - prisoner x and y coordinates\n",
    "        - guard x and y coordinates\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - rewards\n",
    "        - timestamp\n",
    "        - infos\n",
    "\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        # Execute actions\n",
    "        prisoner_action = actions[\"prisoner\"]\n",
    "        guard_action = actions[\"guard\"]\n",
    "\n",
    "        if prisoner_action == 0 and self.prisoner_x > 0:\n",
    "            self.prisoner_x -= 1\n",
    "        elif prisoner_action == 1 and self.prisoner_x < 6:\n",
    "            self.prisoner_x += 1\n",
    "        elif prisoner_action == 2 and self.prisoner_y > 0:\n",
    "            self.prisoner_y -= 1\n",
    "        elif prisoner_action == 3 and self.prisoner_y < 6:\n",
    "            self.prisoner_y += 1\n",
    "\n",
    "        if guard_action == 0 and self.guard_x > 0:\n",
    "            self.guard_x -= 1\n",
    "        elif guard_action == 1 and self.guard_x < 6:\n",
    "            self.guard_x += 1\n",
    "        elif guard_action == 2 and self.guard_y > 0:\n",
    "            self.guard_y -= 1\n",
    "        elif guard_action == 3 and self.guard_y < 6:\n",
    "            self.guard_y += 1\n",
    "\n",
    "        # Check termination conditions\n",
    "        terminations = {a: False for a in self.agents}\n",
    "        rewards = {a: 0 for a in self.agents}\n",
    "        if self.prisoner_x == self.guard_x and self.prisoner_y == self.guard_y:\n",
    "            rewards = {\"prisoner\": -1, \"guard\": 1}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "\n",
    "        elif self.prisoner_x == self.escape_x and self.prisoner_y == self.escape_y:\n",
    "            rewards = {\"prisoner\": 1, \"guard\": -1}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "\n",
    "        # Check truncation conditions (overwrites termination conditions)\n",
    "        truncations = {a: False for a in self.agents}\n",
    "        if self.timestep > 100:\n",
    "            rewards = {\"prisoner\": 0, \"guard\": 0}\n",
    "            truncations = {\"prisoner\": True, \"guard\": True}\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        # Get observations\n",
    "        observations = {\n",
    "            a: (\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y,\n",
    "            )\n",
    "            for a in self.agents\n",
    "        }\n",
    "\n",
    "        # Get dummy infos (not used in this example)\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        # if any(terminations.values()) or any(truncations.values()):\n",
    "            # self.agents = []\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the environment.\"\"\"\n",
    "        grid = np.full((7, 7), \" \")\n",
    "        grid[self.prisoner_y, self.prisoner_x] = \"P\"\n",
    "        grid[self.guard_y, self.guard_x] = \"G\"\n",
    "        grid[self.escape_y, self.escape_x] = \"E\"\n",
    "        print(f\"{grid} \\n\")\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return spaces.Box(low=0, high=48, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_marl = MRLEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env_marl)\n",
    "env = supersuit.concat_vec_envs_v1(env,4,num_cpus=1, base_class=\"stable_baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(MlpPolicy,env,verbose=1,learning_rate=1e-4,batch_size=256,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 8850  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 1     |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1998aead0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils import parallel_to_aec\n",
    "\n",
    "aec_env = parallel_to_aec(env_marl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 0\n",
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 2\n",
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 4\n",
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 6\n",
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 8\n",
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 10\n",
      "guard_reward 0\n",
      "[['P' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 12\n",
      "guard_reward 0\n",
      "[[' ' 'P' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 14\n",
      "guard_reward 0\n",
      "[[' ' ' ' 'P' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[[' ' ' ' ' ' 'P' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 16\n",
      "guard_reward 0\n",
      "[[' ' ' ' ' ' 'P' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[[' ' ' ' ' ' ' ' 'P' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 18\n",
      "guard_reward 0\n",
      "[[' ' ' ' ' ' ' ' 'P' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[[' ' ' ' ' ' ' ' ' ' 'P' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 20\n",
      "guard_reward 0\n",
      "[[' ' ' ' ' ' ' ' ' ' 'P' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "prisoner_reward 0\n",
      "[[' ' ' ' ' ' ' ' ' ' ' ' 'G']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' 'E' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']] \n",
      "\n",
      "step 22\n",
      "guard_reward 1\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m truncations \u001b[39mor\u001b[39;00m terminations:\n\u001b[1;32m      8\u001b[0m   act \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m aec_env\u001b[39m.\u001b[39;49mstep(act)\n\u001b[1;32m     11\u001b[0m aec_env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m agent \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mprisoner\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/rl_scheduling/rlenv/lib/python3.10/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:96\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/rl_scheduling/rlenv/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/rl_scheduling/rlenv/lib/python3.10/site-packages/pettingzoo/utils/conversions.py:357\u001b[0m, in \u001b[0;36mparallel_to_aec_wrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminations[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection]\n\u001b[1;32m    354\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtruncations[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection]\n\u001b[1;32m    355\u001b[0m ):\n\u001b[1;32m    356\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actions[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection]\n\u001b[0;32m--> 357\u001b[0m     \u001b[39massert\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_was_dead_step(action)\n\u001b[1;32m    359\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aec_env.reset()\n",
    "step = 0\n",
    "for agent in aec_env.agent_iter():\n",
    "  obs, rewards, terminations, truncations, infos = aec_env.last()\n",
    "  print(f'{agent}_reward',rewards)\n",
    "\n",
    "  if not truncations or terminations:\n",
    "    act = model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "  aec_env.step(act)\n",
    "  aec_env.render()\n",
    "\n",
    "  if agent == 'prisoner':\n",
    "    print('step',step)\n",
    "    step = step + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
